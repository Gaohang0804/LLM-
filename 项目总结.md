

较熟悉python、Golang，pytorch、transformer、使用过Vit、DETR、LLama大语言模型、whisper、Bert、k8s等。有基于lora的大模型微调的基础经验。

# 项目总结

## 舆情分析

从0-1完成舆情分析项目搭建开发

项目介绍

​	使用scrapy、selenium等完成对主流媒体的公司舆情信息爬取，经过数据清洗后，输入至BaiChuan(7B)大模型中进行分析，完成部分网站爬虫开发，大模型prompt开发，使用openLLM大模型框架提升模型吞吐量，针对视频、音频数据，使用whisper(1.5B)模型完成音频转文字任务，并输入至语言大模型。使用Go语言的gin服务框架完成模型调用服务开发，并将模型部署至k8s。*基于lora等完成对大模型的微调工作*

LLama、LLM、多模态、openLLM、whisper、prompt、lora、scrapy、selenium、Golang、gin、k8s



### 爬虫

#### scrapy、selenium的使用

#### 不重复爬取

#### 定时任务

#### 随机UA

基于scrapy框架提供的中间件模块，完成不重复爬取中间件的开发，在每次启动服务时，会从S3中读取到已经爬取到的url的json发送request请求时，



### 数据清洗、处理

#### spark、数据清洗集群



### 语音转文字模型

#### whisper模型原理

#### whisper模型部署



#### whisper模型微调

#### whisper中间件（golang）

### LLM

#### LLM框架

#### LLM部署

#### LLM中间件（golang）

#### LLM微调

















## 场景挖掘

### 项目架构

#### 执行流程



### Vit原理及源码

### Vi-DETR









## HuggingFace源码阅读

















## 多模态基础

### Zero-shot





### CLIP









## LLM基础

### 微调

#### Lora

Adapter Tuning存在训练和推理延迟，Prefix Tuning难训且会减少原始训练数据中的有效文字长度







## NLP基础

### Transformer

​	attention：Scaled-dot-product Attention

​	1、分词（tokenize）：[CLS] The boy is sleeping.[SEP]

  				input_ids : [101, 464, 2993, 11029, 13, 102]

  2、词嵌入（embedding）：分词变成词向量：word2vec（同一个单词embedding是固定的）、GloVe、BERT（同一个单词embedding会根据所在语句不同而

不同）



原句：                                               Your                       cat   is   a   lovely   cat.

Embedding(原文中是512维度):  （1， 512）

位置编码： 也是512 与embedding相加

![image-20240122155427957](./images/image-20240122155427957.png)

#### 绝对位置编码

在求位置编码时，会针对句子中每个词的pos和每个词的512个维度分别利用下面这个公式计算，偶数个用sin奇数用cos

![image-20240122160027476](./images/image-20240122160027476.png)





#### 注意力机制

假设我们输入的句子有6个token，那么我们要得到它的QKV，就是简单地给这个6x512矩阵复制三份出来，注意这个不是参数矩阵，而是将embedding编码成QKV。

下面那个6x6矩阵就是QxK^T

这个注意力机制为什么叫Scaled-xxx-attention，因为除了根号下dk



为什么要除以根号下 dk（dk 表示 Key 的维度），这是为了缩放（scale）注意力权重，以确保在点乘的过程中不会因为维度较大而导致梯度变得过小，从而影响模型的训练稳定性。



![image-20240122160806444](./images/image-20240122160806444.png)





#### Multi-head

这里会把QKV和对应的W参数矩阵相乘之后再分出多个head，目的是可以让不同的head学习到不同的信息，关注不同的token。

![image-20240122161542110](./images/image-20240122161542110.png)





#### Add&Norm

让训练更加稳定，类似于dropout

add指的是残差操作

batchNorm是在一个batch中的不同样本对应位置之间做normalization，LayerNorm是同一层，也就是一个batch内部做normalization，使得其均值为0方差为1。

![image-20240122163434616](./images/image-20240122163434616.png)



#### Masked-Multi-head Attention

添加mask是在QK^T的位置添加，把mask掉的地方初始化成-inf，这样在做softmax时直接就变成0了（e^−∞=0）

![image-20240122164558671](./images/image-20240122164558671.png)





#### self-Attention与cross-Attention

self-Attention是一个句子在句子内部做的attention，cross-Attention是跨句子的attention



#### 训练过程

encoder输入翻译前的句子，decoder输入翻译结果。

decoder输入会右移一位：

确保在解码时，每个位置的输出只与它前面的位置的输出相关，而不受到当前位置之后的影响。

在实际操作中，右移一位的操作通常是通过在序列的开始添加一个特殊的起始符号（例如，开始符号 `<sos>`）来实现的。这样，模型的第一个输入就是 `<sos>` 符号，而实际的目标序列从第二个位置开始。

![image-20240122170349429](./images/image-20240122170349429.png)







#### 推理过程

推理时，第一次输入<SoS>，第二次接着输入“<SoS>我”，依次下去，直到推理<EoS>。

这种decoder又称为causalDecoder（因果），该解码器是自回归的（autoregressive）。自回归模型是一种在生成序列时一个接一个地生成每个元素的模型。

"Causal" 表示因果关系，因此 "causal decoder" 意味着在生成输出序列的每个时间步时，模型只能依赖于之前的已生成部分，而不能依赖于未来的部分。这是通过确保在每个时间步只使用已生成的部分作为上下文来实现的。

 通常通过在每个解码器的自注意力机制（self-attention mechanism）中应用一个掩码（mask）来实现。

![image-20240122171634100](./images/image-20240122171634100.png)



#### 自回归模型


自回归模型是一种生成模型，其生成的序列是按照时间步顺序逐个元素生成的模型。在自回归模型中，生成的每个元素依赖于前面已生成的元素。

简而言之，自回归模型通过考虑序列中先前的元素来生成下一个元素。这种模型通常用于处理时序数据或序列数据，如文本生成、语音合成等任务。





#### Transformer的优化

Transformer的时间复杂度

On^2，其中n为输入序列长度

![image-20240126161258137](./images/image-20240126161258137.png)









### BERT/GPT/BART

Masked language modeling：mask掉一个词，然后去预测这个词。



![image-20240122210911971](./images/image-20240122210911971.png)





