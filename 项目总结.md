



较熟悉python、Golang，pytorch、transformer、使用过Vit、DETR、LLama大语言模型、whisper、Bert、k8s等。有基于lora的大模型微调的基础经验。

# 项目总结

## 舆情分析

从0-1完成舆情分析项目搭建开发

项目介绍

​	使用scrapy、selenium等完成对主流媒体的公司舆情信息爬取，经过数据清洗后，输入至BaiChuan(7B)大模型中进行分析，完成部分网站爬虫开发，大模型prompt开发，使用openLLM大模型框架提升模型吞吐量，针对视频、音频数据，使用whisper(1.5B)模型完成音频转文字任务，并输入至语言大模型。使用Go语言的gin服务框架完成模型调用服务开发，并将模型部署至k8s。*基于lora等完成对大模型的微调工作*

LLama、LLM、多模态、openLLM、whisper、prompt、lora、scrapy、selenium、Golang、gin、k8s



### 爬虫

#### scrapy、selenium的使用

#### 不重复爬取

#### 定时任务

#### 随机UA

基于scrapy框架提供的中间件模块，完成不重复爬取中间件的开发，在每次启动服务时，会从S3中读取到已经爬取到的url的json发送request请求时，



### 数据清洗、处理

#### spark、数据清洗集群



### 语音转文字模型

#### whisper模型原理

#### whisper模型部署



#### whisper模型微调

#### whisper中间件（golang）

### LLM

#### 通义千问

项目目录

![image-20240130134331140](images/image-20240130134331140.png)



分词采用tiktoken

https://blog.csdn.net/jclian91/article/details/130959300

`tiktoken`是OpenAI开源的Python第三方模块，该模块主要实现了tokenizer的BPE（Byte pair encoding）算法，并对运行性能做了极大的优化。本文将介绍tiktoken模块的使用。



推理阶段控制参数：

```json
{
  "chat_format": "chatml",  # 对话格式类型 问答对
  "eos_token_id": 151643,
  "pad_token_id": 151643,
  "max_window_size": 6144,
  "max_new_tokens": 512,
  "do_sample": true,
  "top_k": 0,
  "top_p": 0.8,
  "repetition_penalty": 1.1,
  "transformers_version": "4.31.0"
}
```

xxx-预训练和xxx-chat的区别

预训练：给予前文预测后文

聊天：固定格式的问答对

<|im_start|> 后面跟角色，system、user、assistant.....

system也可以改，比如改成“你是一个二次元角色”

![image-20240130154750333](images/image-20240130154750333.png)

##### history堆叠格式

![image-20240130160442771](images/image-20240130160442771.png)

##### prompt的结构

![image-20240130161229076](images/image-20240130161229076.png)





##### 通义千问SFT微调

https://github.com/owenliang/qwen-sft

我们通过SFT微调，来解决下面这个模型胡说的问题

开始胡说了，我只让它输出文本中的城市和日期，它却输出了天气

prompt：提示词工程，使用%s作为占位符

![image-20240130165717713](images/image-20240130165717713.png)

###### 生成微调数据

qwen的微调数据格式

```
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "你好"
      },
      {
        "from": "assistant",
        "value": "我是一个语言模型，我叫通义千问。"
      }
    ]
  }
]
```

我们把准备好的随机城市和随机日期数据格式化成下面这个json，这个json里的数据就是我们微调要用的

![image-20240130171103283](images/image-20240130171103283.png)



![image-20240131095224112](images/image-20240131095224112.png)





###### 微调

可以查看Qwen/example中的说明

微调直接执行qwen funtune文件夹下的脚本 

```
Qwen/finetune/finetune_lora_single_gpu.sh
```

```shell
#!/bin/bash
export CUDA_DEVICE_MAX_CONNECTIONS=1

MODEL="Qwen/Qwen-7B" # Set the path if you do not want to load from huggingface directly
# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.
# See the section for finetuning in README for more information.
DATA="path_to_data"

function usage() {
    echo '
Usage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]
'
}

while [[ "$1" != "" ]]; do
    case $1 in
        -m | --model )
            shift
            MODEL=$1
            ;;
        -d | --data )
            shift
            DATA=$1
            ;;
        -h | --help )
            usage
            exit 0
            ;;
        * )
            echo "Unknown argument ${1}"
            exit 1
            ;;
    esac
    shift
done

export CUDA_VISIBLE_DEVICES=0

python finetune.py \
  --model_name_or_path $MODEL \
  --data_path $DATA \
  --bf16 True \
  --output_dir output_qwen \
  --num_train_epochs 5 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 1 \
  --gradient_accumulation_steps 8 \
  --evaluation_strategy "no" \
  --save_strategy "steps" \
  --save_steps 1000 \
  --save_total_limit 10 \
  --learning_rate 3e-4 \
  --weight_decay 0.1 \
  --adam_beta2 0.95 \
  --warmup_ratio 0.01 \
  --lr_scheduler_type "cosine" \
  --logging_steps 1 \
  --report_to "none" \
  --model_max_length 512 \
  --lazy_preprocess True \
  --gradient_checkpointing \
  --use_lora

# If you use fp16 instead of bf16, you should use deepspeed
# --fp16 True --deepspeed finetune/ds_config_zero2.json
```



再回答我们的提取日期城市问题，就很稳定

![image-20240130172742942](images/image-20240130172742942.png)



#### DeepSpeed

多卡训练，模型部署框架

```shell
pip3 install deepspeed
```

它会自己调度使用哪个GPU做哪个训练，将我们的参数放在不同的GPU上。

多卡通信使用nccl

##### 分布式训练

![image-20240131103427666](images/image-20240131103427666.png)

执行前调用

![image-20240131101045343](images/image-20240131101045343.png)

四卡      dist_world_size = 4

CUDA_VISIABLE_DEVICES=0,1,2,3

![image-20240131101247538](images/image-20240131101247538.png)



开四个进程

![image-20240131101510105](images/image-20240131101510105.png)



训练结果分布式落盘

![image-20240131101550822](images/image-20240131101550822.png)





##### 分布式推理

每个GPU都要执行加载ckpt的操作，只有主进程

![image-20240131102552515](images/image-20240131102552515.png)

![image-20240131103039276](images/image-20240131103039276.png)





#### LLM框架









#### LLM部署

#### LLM中间件（golang）

#### LLM微调



















## 场景挖掘

### 项目架构

#### 执行流程



### Vit原理及源码

### Vi-DETR









## HuggingFace源码阅读

















## 多模态基础

### Zero-shot





### CLIP









## LLM基础

### Encoder-Decoder与Decoder-only

Decoder-only把一个问答结构改成了续写结构

不仅是翻译，我们只需要在输入时错开，向右移一位

DecoderOnly结构更简单，显著减小了模型的大小，且可以使用无标签数据进行训练（找个句子，错开一位，让它预测下一位token）

![image-20240131105025904](images/image-20240131105025904.png)

对于通义千问，输入是蓝色选中部分，基于这一部分进行后面的推理

![image-20240131105320688](images/image-20240131105320688.png)



![image-20240131104224197](images/image-20240131104224197.png)





### 大模型推理与部署

模型推理，生成和分类用的不同的推理方式

对于分类模型，我们只需要取最开头那个 [CLS] token

![image-20240129132528029](images/image-20240129132528029.png)



任务范式的转移，分类任务变成生成任务



#### 解码策略

![image-20240129134005665](images/image-20240129134005665.png)

##### 贪心搜索GreedySearch

哪个概率高走那条路径

缺点：找到的并不一定是概率值最高的。

![image-20240129134808119](images/image-20240129134808119.png)

##### 柱搜索BeamSearch

设置超参选择柱数量，用于储存前beam个概率最大的结果



##### 让每次生成的结果有所不同--Top-KSampling

需结合BeamSearch使用

![image-20240129135541002](images/image-20240129135541002.png)





我们可以发现已知词越多，后面的概率值越高。

**因为是取前K个，如果这里面有概率值极小的，生成的句子也会不合理**

**Top-K的问题就是，它只关注了前K个概率，但是没关注概率分布，概率值是多少**

![image-20240129135753200](images/image-20240129135753200.png)



##### Top-PSampling

找**概率和**为95%的数据拿出来，而不是固定前几个

实际上还是使用Top-K更多些

![image-20240129140306736](images/image-20240129140306736.png)

##### 防止说重复的话--对比搜索

degenernation 惩罚项，让它与之前已经生成的结果做相似度计算

![image-20240129143542177](images/image-20240129143542177.png)



##### 让结果生成的更快--AssistGenernation

先让小模型生成结果，大模型评价，大模型觉得生成的结果不满足要求再自己生成

![image-20240129144317093](images/image-20240129144317093.png)



#### 把之前计算的结果保存下来--KVCache

将KV结果矩阵存在内存里，让我们的大模型生成速度更快

attention计算时，只计算新来的embedding

![image-20240129144808194](images/image-20240129144808194.png)



### 放到不同的GPU里--并行化

#### pipline Parallelism

pipline并行，或放在不同的GPU里

![image-20240129145430620](images/image-20240129145430620.png)



#### Tensor Parallelism

![image-20240129145724991](images/image-20240129145724991.png)





#### Sequence Paralleism

对于顺序执行的部分，我们可以看成一个整体的模块然后并行计算

![image-20240129145920181](images/image-20240129145920181.png)





在推理阶段依旧可以多GPU并行计算

### Attention优化

多个Query对应一组KV，提升Attention计算速度

![image-20240129150238506](images/image-20240129150238506.png)





### Flash attention

内存角度优化计算速度，将多个模块的操作打包成kernel，避免数据传输消耗的时间

![image-20240129150918921](images/image-20240129150918921.png)



### 模型量化

通过模型的量化，可以将模型中的参数和激活值从浮点数转换为较低位宽的定点数或者整数。这种技术的目标是在尽可能保持模型性能的前提下，减小模型的存储需求和计算复杂度。

![image-20240129151649687](images/image-20240129151649687.png)









### 大模型测评方法

两种：人直接看（或者找个更厉害的模型，如GPT-4）；多选，选择哪个答案更好

抽取答案：Extract Match

![image-20240130104235855](images/image-20240130104235855.png)





### 思维链Chain-of-Thought

思维链就是一步一步去思考，让模型有这种一步一步思考的能力

NLP研究范式：

Fine-tuning 和 Instruction tuning都需要训练

![image-20240130111934832](images/image-20240130111934832.png)



几篇CoT的重要文章

![image-20240130112212455](images/image-20240130112212455.png)









### 微调

我们可以获取到的开源模型可能是基于公开数据进行训练的，我们可能需要针对让它更适用于我们企业内的情况，这时我们可能需要进行微调



#### SFT

#### Lora

https://github.com/owenliang/pytorch-diffusion/tree/main

Adapter Tuning存在训练和推理延迟，Prefix Tuning难训且会减少原始训练数据中的有效文字长度

W和WaWb结果相加给下一层，但是W被冻结，只有WaWb反向传播

Wa给原来的权重矩阵维度降到r，然后再用Wb恢复，以此降低计算了

Wa: d x r； Wb:  r x d

在完成Wa，Wb的反向传播后，我们会把它俩相乘，然后合到冻结的原始权重中

![image-20240131111451004](images/image-20240131111451004.png)



初始化Wa为随机分布，Wb为0矩阵

![image-20240131112201179](images/image-20240131112201179.png)



对于有Attention的，我们一般会使用lora替换Wq、Wk、Wv这些参数矩阵







### vllm 

https://readpaper.feishu.cn/docx/EcZxdsf4uozCoixdU3NcW03snwV

利用**PagedAttention**技术，该库通过有效地管理**Attention**模块中的**Key**和**Value**的**Cache**，重新定义了LLM的推理服务。无需更改任何模型架构，它的吞吐量比原生HF Transformers高出**24倍**。

#### KVCache

将之前计算的KV值存储起来，下次只计算上**一**次生成的KV

![image-20240131195611409](images/image-20240131195611409.png)



#### Page-Attention

通过KV Cache的技术，我们已经可以极大地提升LLM地推理速度，但是现有的Cache仍存在一些问题，

- ***Large***：对于LLaMA-13B中的单个序列，它占用高达1.7GB的内存。
- ***Dynamic***：它的大小取决于序列长度，而序列长度具有高度可变和不可预测的特点。

因此，高效地管理KV Cache是一个重大挑战。现有系统（HuggingFace 默认实现是pytorch的内存分配策略）由于内存碎片化和过度预留而浪费了60%至80%的内存。

为了解决这个问题，我们引入了PagedAttention，PagedAttention允许将连续的键和值存储在非连续的内存空间中。具体而言，PagedAttention将每个序列的KV缓存分成多个块，每个块包含固定数量的标记的键和值。在注意力计算过程中，PagedAttention Kernel高效地识别和获取这些块，采用并行的方式加速计算。



在PagedAttention中，内存浪费仅发生在序列的最后一个块中。这样就使得我们的方案接近最优的内存使用率，仅有不到4%的浪费。通过内存效率的提升，我们能够显著提升BatchSize，同时进行多个序列的推理，提高GPU利用率，从而显著提高吞吐量。



内存共享： PagedAttention中的不同序列可以通过将它们的逻辑块映射到相同的物理块来共享块。



Page-Attention根据一个维护的映射表（Block table）来将逻辑上的KV存储位置与物理上的存储位置对应。

![image-20240131200822052](images/image-20240131200822052.png)











## NLP基础

### Transformer

​	attention：Scaled-dot-product Attention

​	1、分词（tokenize）：[CLS] The boy is sleeping.[SEP]

  				input_ids : [101, 464, 2993, 11029, 13, 102]

  2、词嵌入（embedding）：分词变成词向量：word2vec（同一个单词embedding是固定的）、GloVe、BERT（同一个单词embedding会根据所在语句不同而

不同）



原句：                                               Your                       cat   is   a   lovely   cat.

Embedding(原文中是512维度):  （1， 512）

位置编码： 也是512 与embedding相加

![image-20240122155427957](images/image-20240122155427957.png)

#### 绝对位置编码

在求位置编码时，会针对句子中每个词的pos和每个词的512个维度分别利用下面这个公式计算，偶数个用sin奇数用cos

![image-20240122160027476](images/image-20240122160027476.png)





#### 注意力机制

假设我们输入的句子有6个token，那么我们要得到它的QKV，就是简单地给这个6x512矩阵复制三份出来，注意这个不是参数矩阵，而是将embedding编码成QKV。

下面那个6x6矩阵就是QxK^T

这个注意力机制为什么叫Scaled-xxx-attention，因为除了根号下dk



为什么要除以根号下 dk（dk 表示 Key 的维度），这是为了缩放（scale）注意力权重，以确保在点乘的过程中不会因为维度较大而导致梯度变得过小，从而影响模型的训练稳定性。



![image-20240122160806444](images/image-20240122160806444.png)





#### Multi-head

这里会把QKV和对应的W参数矩阵相乘之后再分出多个head，目的是可以让不同的head学习到不同的信息，关注不同的token。

![image-20240122161542110](images/image-20240122161542110.png)





#### Add&Norm

让训练更加稳定，类似于dropout

add指的是残差操作

batchNorm是在一个batch中的不同样本对应位置之间做normalization，LayerNorm是同一层，也就是一个batch内部做normalization，使得其均值为0方差为1。

![image-20240122163434616](images/image-20240122163434616.png)



#### Masked-Multi-head Attention

添加mask是在QK^T的位置添加，把mask掉的地方初始化成-inf，这样在做softmax时直接就变成0了（e^−∞=0）

![image-20240122164558671](images/image-20240122164558671.png)





#### self-Attention与cross-Attention

self-Attention是一个句子在句子内部做的attention，cross-Attention是跨句子的attention



#### 训练过程

encoder输入翻译前的句子，decoder输入翻译结果。

decoder输入会右移一位：

确保在解码时，每个位置的输出只与它前面的位置的输出相关，而不受到当前位置之后的影响。

在实际操作中，右移一位的操作通常是通过在序列的开始添加一个特殊的起始符号（例如，开始符号 `<sos>`）来实现的。这样，模型的第一个输入就是 `<sos>` 符号，而实际的目标序列从第二个位置开始。

![image-20240122170349429](images/image-20240122170349429.png)







#### 推理过程

推理时，第一次输入<SoS>，第二次接着输入“<SoS>我”，依次下去，直到推理<EoS>。

这种decoder又称为causalDecoder（因果），该解码器是自回归的（autoregressive）。自回归模型是一种在生成序列时一个接一个地生成每个元素的模型。

"Causal" 表示因果关系，因此 "causal decoder" 意味着在生成输出序列的每个时间步时，模型只能依赖于之前的已生成部分，而不能依赖于未来的部分。这是通过确保在每个时间步只使用已生成的部分作为上下文来实现的。

 通常通过在每个解码器的自注意力机制（self-attention mechanism）中应用一个掩码（mask）来实现。

![image-20240122171634100](images/image-20240122171634100.png)



#### 自回归模型


自回归模型是一种生成模型，其生成的序列是按照时间步顺序逐个元素生成的模型。在自回归模型中，生成的每个元素依赖于前面已生成的元素。

简而言之，自回归模型通过考虑序列中先前的元素来生成下一个元素。这种模型通常用于处理时序数据或序列数据，如文本生成、语音合成等任务。





#### Transformer的优化

Transformer的时间复杂度

On^2，其中n为输入序列长度

Low-Rank，使用低秩矩阵进行近似计算

![image-20240126161258137](images/image-20240126161258137.png)





##### 稀疏矩阵方法--减小计算量

Sliding window attention 每个token只看与其临近的几个token的attention。

Dilated：隔一个算一次attention



BigBird：full-attention + windowsized-attention  +random-attention

![image-20240129105618297](images/image-20240129105618297.png)



![image-20240129110423554](images/image-20240129110423554.png)

##### 推理与压缩

剪枝、蒸馏、量化

给强模型剪枝，使其剪枝后的模型输出尽可能接近强模型

![image-20240129111011219](images/image-20240129111011219.png)



### BERT/GPT/BART

Masked language modeling：mask掉一个词，然后去预测这个词。



![image-20240122210911971](images/image-20240122210911971.png)













