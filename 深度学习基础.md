## AP和mAP

AP和mAP是深度学习中用于评估目标检测模型性能的两个常用指标。AP是平均精度（Average Precision）的缩写，而mAP则是平均精度的平均值（mean Average Precision）。  

 AP是一种**用于评估单一类别目标**检测模型性能的指标，其计算方式是在不同阈值下计算模型的精确度和召回率，并计算出不同阈值下的面积。最后，将这些面积加权平均得到AP结果。AP越高，表示模型在该类别目标检测上的性能越好。  

而mAP则是**用于评估多类别目标**检测模型性能的指标。在多类别目标检测中，通常需要对每个类别单独计算AP，并将所有类别的AP取平均值得到mAP结果。mAP越高，表示模型在多类别目标检测上的性能越好。  

总之，AP和mAP是深度学习中常用的目标检测模型评估指标，分别用于评估单一类别和多类别目标检测模型的性能。



## 1x1卷积核的作用

https://blog.csdn.net/m0_47146037/article/details/127769028

### 1、增加网络深度（增加非线性映射次数）

可以在不减小特征图维度的情况下引入非线性。

我们都知道卷积层数越深，感受野越大，获得的特征越抽象，有的时候，我们想在不增加感受野的情况下，让网络加深，为的就是引入更多的非线性，增加神经网络的表达能力。

如果卷积核是 1x1 ，跨度也是 1，那么生成后的图像大小就并没有变化。

### 2、升维/降维

降维

用卷积核的通道数与特征图的通道数做卷积，**特征图多余的通道数舍弃不做计算**(参数已经很多了，太多有可能过拟合)

在实践中，通道数的下降通常不会对模型的准确率产生太大的影响，而且可以大大提高模型的效率。但是，如果我们将通道数降低得太多，可能会丢失一些关键的特征信息，从而影响模型的准确率。

![在这里插入图片描述](https://img-blog.csdnimg.cn/899075686f6041739715f7b34869337d.png)

升维

用多个卷积核与一组特征图做卷积计算

使用1x1卷积核做卷积的原因是它的参数量最小

### 减少卷积核参数（简化模型）



## 空洞卷积

https://blog.csdn.net/mrjkzhangma/article/details/104929302

加入空洞之后的实际卷积核尺寸与原始卷积核尺寸之间的关系：**K = K + (k-1)(a-1)**其中**k为原始卷积核大小，a为卷积扩张率(dilation rate)**，

## distill

通过在softmax函数中引入温度参数，来控制输出的概率分布的平滑程度。具体来说，温度参数T越大，输出的概率分布就越平滑，即更加均匀。



## 余弦相似度

https://blog.csdn.net/u014539465/article/details/105353638/

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTEwNjE2LzIwMTgwOC8xMTA2MTYtMjAxODA4MzEwNjQxMTAwNzQtNDM1NDA2ODU5LnBuZw?x-oss-process=image/format,png)

余弦相似度算法：一个向量空间中两个向量夹角间的余弦值作为衡量两个个体之间差异的大小，余弦值接近1，夹角趋于0，表明两个向量越相似，余弦值接近于0，夹角趋于90度，表明两个向量越不相似。





## 深度学习中的backbone、neck、head

https://zhuanlan.zhihu.com/p/607578342

backbone：模型主要部分，负责特征提取，输入先经过backbone，它是整个模型结构参数最多的部分。

neck：用来连接backbone和head，主要是对backbone的输出进行降维、调整等处理。

head：最后一层，通常是一个分类器或回归器。head 通过输入经过 neck 处理过的特征，产生最终的输出。对于图像分类任务，可以使用 softmax 分类器；对于目标检测任务，可以使用边界框回归器和分类器等。





## 最基本的迁移学习demo

https://blog.csdn.net/weixin_41466575/article/details/119697986

将冻结的层的requires_grad设置为False，或者指定冻结层的学习率为0

```python
# 可以通过如下方式对不需要训练的网络进行冻结
for p in resnet18.parameters():
    p.requires_grad = False
for layer in [resnet18.layer4.parameters(),resnet18.fc.parameters()]:
  for p in layer:
    p.requires_grad = True
params_non_frozen = filter(lambda p: p.requires_grad, resnet18.parameters())
opt = optim.SGD(params_non_frozen, lr=learning_rate, momentum=0.9)


# 另外一种冻结的方式
# opt = torch.optim.SGD([
#     dict(params=resnet18.layer1.parameters(), lr=0),
#     dict(params=resnet18.layer2.parameters(), lr=0),
#     dict(params=resnet18.layer3.parameters(), lr=0),
#     dict(params=resnet18.layer4.parameters(), lr=1e-4),
#     # layer4这一层微微调一下就行，所以学习率很小
#     dict(params=resnet18.fc.parameters()),
#     # fc这一层微调，使用0.01学习率
# ], lr=learning_rate, momentum=0.9)
```



## 迁移学习与域自适应？？？

​	迁移学习和域自适应的区别在于，迁移学习是指利用从一个任务中获得的知识来提高在另一个相关任务上的性能，而域自适应是指将在一个领域中训练的模型适应到另一个相关领域中以获得良好的性能。
 	在迁移学习中，预训练模型被用作新任务的起点。**预训练模型已经从大量数据中学习到了有用的特征，这些特征可以在进行一些修改后用于新任务**。例如，使用在自然图像数据集上训练的模型作为新任务的起点，进行特定物体在医学图像中的识别。
 	在域自适应中，目标是将在源域中训练的模型适应到在目标域中良好地表现。源域和目标域相关，但可能具有不同的分布或特征。例如，使用在一个国家的数据上训练的模型在另一个国家的数据上表现不佳，这是由于语言、文化或其他因素的差异造成的。域自适应技术旨在通过修改模型以更好地适应目标域来弥合这种差距。



## 数学期望

https://www.zhihu.com/question/20994432/answer/2328255891

简单来说，期望就是加权平均值形式

要么是积分要么是求和（看连续还是离散），本质上都是求和

在某些情况下，例如每个事件出现的概率均等的模型下，期望 = 平均数

![image-20230518202158164](/home/zee001-w/.config/Typora/typora-user-images/image-20230518202158164.png)

![image-20230518202213302](/home/zee001-w/.config/Typora/typora-user-images/image-20230518202213302.png)





## 朴素贝叶斯

https://blog.csdn.net/weixin_44992737/article/details/127145702

朴素贝叶斯算法在假设n个特征独立分布的情况下，最终算的是一系列概率（二分类、多分类），这些概率与样本的特征有关，如果有N个类别就需要计算N个概率，其中概率最大的就是样本所属类别。

![7f932f40c5744bd8bc72aa600a2228ce.png](https://img-blog.csdnimg.cn/7f932f40c5744bd8bc72aa600a2228ce.png)

### 对数的运算法则

​	加法法则：log(a*b) = loga + logb

​	减法法则：log(a/b) = loga - logb

![img](https://pics2.baidu.com/feed/f9dcd100baa1cd115358dc17bf4fcbf0c2ce2d87.jpeg@f_auto?token=7b3c184df73ea3c86f7cd7a73489d754)	



## 协方差

https://zhuanlan.zhihu.com/p/382226938

协方差（convariance）在概率论和统计学中用于衡量两个变量的总体误差,在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。

![image-20230518203538068](/home/zee001-w/.config/Typora/typora-user-images/image-20230518203538068.png)

协方差矩阵

![img](https://pic2.zhimg.com/80/v2-3980b45f4efe2d40eb8c4cd298a3e381_720w.jpg)





## 梯度消失与梯度下降

https://blog.csdn.net/hollyprince/article/details/125900787

我们在做反向传播的过程中，有一个从末层向前的权重更新的过程，需要链式法则的帮助，**链式法则是一个连乘的形式**，当层数越深的时候，**梯度将以指数形式传播**。因此当末层的梯度

梯度消失或[梯度爆炸](https://so.csdn.net/so/search?q=梯度爆炸&spm=1001.2101.3001.7020)在本质原理上其实是一样的，

梯度消失：经常出现，产生的原因有：一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。

**【梯度爆炸】**一般出现在**深层网络**和**权值初始化值太大**的情况下。在深层神经网络或循环神经网络中，**误差的梯度可在更新中累积相乘**。如果网络层之间的**梯度值大于 1.0**，那么**重复相乘会导致梯度呈指数级增长**，梯度变的非常大，然后**导致网络权重的大幅更新，并因此使网络变得不稳定**。

如果**接近输出层的激活函数求导后梯度值大于1**，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生**梯度爆炸**；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生**梯度消失**。

梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。

解决方法:

**（1） pre-training+fine-tunning**

此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

**（2） 梯度剪切：对梯度设定阈值**

梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

**（3） 权重正则化**

另外一种解决梯度爆炸的手段是采用权重正则化（weithts  regularization），正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，通过正则化项来限制权重的大小，也可以在一定程度上防止梯度爆炸的发生。比较常见的是 L1 正则和 L2 正则，在各个深度框架中都有相应的API可以使用正则化。

关于 L1 和 L2 正则化的详细内容可以参考我之前的文章——[欠拟合、过拟合及如何防止过拟合](https://zhuanlan.zhihu.com/p/72038532)

**（4） 选择relu等梯度大部分落在常数上的激活函数**

relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

关于relu等激活函数的详细内容可以参考我之前的文章——[温故知新——激活函数及其各自的优缺点](https://zhuanlan.zhihu.com/p/71882757)

**（5） batch normalization**

BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

关于Batch Normalization（BN）的详细内容可以参考我之前的文章——[常用的 Normalization 方法：BN、LN、IN、GN](https://zhuanlan.zhihu.com/p/72589565)

**（6） 残差网络的捷径（shortcut）**





## 模型的鲁棒性和泛化性

模型的鲁棒性是指模型在面对干扰或异常情况时，仍然能够保持良好的性能表现，即具有较强的抗扰能力和稳定性，例如在图片中添加噪声后还能成功识别。

模型的泛化性是指模型**在面对未见过的数据时的表现能力**（测试集），即对于新数据的预测能力。泛化能力越强，则模型越能够适应不同的数据集，能够更准确地对未知数据进行预测。

​	假设你正在训练一个模型，该模型旨在自动识别图片中的动物。在训练数据集中，该模型学习了各种不同种类的动物图片，包括猫、狗、熊等等，并且在测试数据集上取得了很好的成绩，但现在你需要部署该模型到实际应用中，在实际应用中，你面临的情况可能与训练数据不同。

举个例子，你在实际应用时可能会面临一些与训练数据不同的“干扰”，比如图片被裁剪、缩放或扭曲，或者包含了一些噪声。而这些“干扰”会对模型性能产生负面影响，例如导致模型产生错误的预测结果。这种情况就反映了模型在鲁棒性方面的表现。

为了提高模型的鲁棒性，可以考虑使用数据增强的技术来模拟各种不同的“干扰”情况，例如对训练数据进行随机裁剪、旋转、变形或加入噪声等手段，从而使模型具有更强的适应性，在新的、未见过的数据上表现更加稳定和可靠。

另外，对于特殊情况（如对抗攻击），可以考虑使用对抗训练等方法，使模型具有更好的防御能力，提高模型的鲁棒性。这些方法都旨在提高模型的鲁棒性，使其在面对各种不同情况时都能够保持良好的性能表现。



以下是几种提升鲁棒性和泛化性的方法：
 1. 扩充训练数据集：增加模型训练时使用的数据集数量和多样性有助于提高模型的泛化性，因为模型能够更好地适应不同的数据分布。同时，数据扩充也可以使模型更具鲁棒性，因为它能够学习到更多的数据变化模式。
 2. 采用正则化方法：通过L1或L2正则化方法，可以对模型权重进行约束，减少模型过拟合的风险，提高模型的泛化性能，从而使模型更具鲁棒性。
 3. 增加噪声鲁棒性：添加噪声或扰动样本可以提升模型对于干扰的鲁棒性，因为模型能够从噪声中学习到更多的特征和模式。
 4. 使用集成方法：通过使用多个模型并行或串行地组合，可以减少模型的偏差和方差，提高模型的预测准确性和泛化性能。
 5. 优化网络结构：通过调整网络结构、增加或减少层数、改变激活函数、优化学习率等方法，可以提升模型的鲁棒性和泛化性。
 这些方法都可以有效提升模型的鲁棒性和泛化性，但在实际应用中，还需要根据具体场景和数据特点进行调整。





## 常用的激活函数及其优缺点

https://zhuanlan.zhihu.com/p/71882757



## BatchNormalization

Batch normalization（批归一化）是深度神经网络中的一种常见方法，旨在提高训练速度和模型的准确性。它通过标准化神经网络中每一层的输入，即将其重新缩放到均值为0和方差为1的标准正态分布中。这样做有助于减少内部协变量偏移，即输入分布随着训练过程而变化所带来的影响，从而加速了神经网络的训练和提高了其精度。

Batch normalization一般加在模型结构的卷积或全连接层之后，活性函数之前。加在卷积或全连接层之后是因为在这些层之后，输入的分布会变得不稳定，从而导致效果下降。加在活性函数之前是为了使得输出的分布更符合标准正态分布，从而保证较好的效果。

Batch normalization的作用主要有以下几点：
 1. 加速神经网络的训练：由于归一化使得输入的分布较为稳定，有效地减少了梯度消失问题，因此可以使用更大的学习率来加速神经网络的训练。
 2. 减少对初始权重的依赖：由于经过归一化后的输入分布更加稳定，因此神经网络对于初始权重的依赖性更小。
 3. 提高泛化能力：归一化降低了模型对训练数据的依赖，从而提高了泛化能力。
 4. 抑制过拟合：在训练过程中，Batch normalization可以看做是一种正则化方法，通过减少神经元的相互依赖来抑制过拟合。
 5. 具有一定的正则化效果：在训练过程中，Batch normalization在一定程度上对输入进行了正则化，从而有助于减轻过度拟合的问题。

 ![image-20230519095113340](/home/zee001-w/.config/Typora/typora-user-images/image-20230519095113340.png)



## 上采样和下采样

上采样：针对少样本类生成新的数据样本参与训练。下采样：针对多样本的类筛选一部分样本参与训练。



## CNN的缺点

1、实际结果中显示，**CNN对边缘的响应很弱**。这也非常好理解，越靠边缘的像素，因为被卷积次数少，自然在梯度更新时，贡献更少。
2、**CNN只能和临近像素计算相关性**。由于其滑窗卷积的特性，无法对非领域的像素共同计算，例如左上角的像素无法和右下角的像素联合卷积。这就导致了某些空间信息是无法利用的。同时根据MAE论文中所说的，自然图像具有冗余性，即相邻像素点代表的信息是差不多的，所以只计算领域像素无法最大化利用图像特征。

## CNN卷积神经网络输出计算公式



卷积层Conv的输入：高为h、宽为w，卷积核的长宽均为kernel，填充为pad,步长为Stride（长宽可不同，分别计算即可），则卷积层的输出维度为：

![img](https://img2020.cnblogs.com/blog/409175/202003/409175-20200311194107596-876719365.png)

![img](https://img2020.cnblogs.com/blog/409175/202003/409175-20200311194120655-612215038.png)

其中上开下闭开中括号表示向下取整。



## 感受野

https://blog.csdn.net/qq_41076797/article/details/114434415

就是输出的特征图上的一个像素点对应输入图上的像素点区域大小

从前往后和从后往前的计算公式

![image-20230518104743296](/home/zee001-w/.config/Typora/typora-user-images/image-20230518104743296.png)





## 目标检测的Anchor机制

为什么需要先验框？

预测出来的框可能很偏，这就导致我们需要平移和缩放的尺度很大。相当于先大概给你圈个范围，让你模型更容易收敛。

数据中的真实框都是各种各样的，有的可能非常大，有的可能非常偏，数据分布非常不统一，而预测框就更不用说了。这就导致了学习到的偏移量和缩放系数变化大，模型难收敛的问题。所以为了解决这个问题，我们引入了**Anchor**机制。我们将假设我们将所有真实框的长宽都设为128，把这个长宽都为128的框叫做先验框Anchor（确保我们需要调整的尺度不是很大）。

在什么阶段进行先验框匹配？

一般来说，我们都会在经过骨干网络处理的特征层上进行先验框匹配。因为如果在一开始的图片上就进行先验框匹配，那就会有很多先验框，这样计算量就会激增。以Faster RCNN为例，输入图片是600x600的大小，而特征层是38x38的大小。如果在输入图片上进行先验框匹配，那就产生 600 ∗ 600 ∗ 9 = 3 , 240 , 000 600 * 600*9 = 3,240,000 600∗600∗9=3,240,000个先验框，而在特征层上则是 38 ∗ 38 ∗ 9 = 12996 38 * 38 *9 = 12996 38∗38∗9=12996个先验框。





## 全连接层的解释和实现

全连接，可以完成**特征的进一步融合。使得神经网络最终看到的特征是个全局特征（一只猫），而不是局部特征（眼睛或者鼻子）。**

https://zhuanlan.zhihu.com/p/552186222

我们知道卷积是对图像的局部区域进行连接，通过卷积核完成的是感受野内的长宽方向以及channel 方向的数据连接。因此，卷积操作，提取的特征是局部特征。也就是说，卷积是“**不是庐山真面目，只缘身在此山中**”。

而全连接层呢？它的每次完成的是所有channel方向的连接，它看到的是全局特征。全连接是“**不畏浮云遮望眼，自缘身在最高层**”。

除此之外，卷积和全连接在算法上是可以转换的。通常情况下，在进行全连接的计算时，可以把它等效于卷积核为1x1的卷积运算。

https://blog.csdn.net/Y_hero/article/details/88296967

nn.Linear(input_feature_size, output_feature_size, bias)

在做全连接前需要转成二维的Tensor，然后调用nn.Linear()，相当于做了一步Wx+b（Wx1+Wx2+Wx3+....+Wxn+b），获取到所有特征后将它们转到指定size





## IOU、GIOU、DIOU、CIOU

https://blog.csdn.net/weixin_42392454/article/details/111152035

这些IOU都相当于在IOU的基础上增加不同的惩罚项

IOU的缺点：1、对距离不敏感，当真值和预测值的框没有交集时，iou为0，但是不确定距离多远。

![img](https://img-blog.csdnimg.cn/20201229161349411.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5MjQ1NA==,size_16,color_FFFFFF,t_70)![img](https://img-blog.csdnimg.cn/20201229161415302.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5MjQ1NA==,size_16,color_FFFFFF,t_70)

​					  2、无法反映重合效果，仅能反映重合度

![iou](https://img-blog.csdnimg.cn/20201214105434408.png#pic_center)

​					3、把IOU当作loss函数进行优化，则loss=0，没有梯度回传，所以无法进行训练

GIOU:

解决了在IoU作为损失函数时梯度无法计算的问题

GIOU = IOU - （C-(AUB)）/C 

GIOU的取值范围为[-1, 1]

其中C是最小可以包住真值框和预测框的矩形

![img](https://img-blog.csdnimg.cn/20201229161515633.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5MjQ1NA==,size_16,color_FFFFFF,t_70)

缺点：当一个框在另一个框内时，会退化为IOU







## 知识蒸馏

知识蒸馏（Knowledge Distillation）是一种模型压缩技术，用于将一个复杂的模型的知识转移到一个更小、更轻量级的模型中。这个过程可以通过以下步骤来实现：

1. **准备教师模型：** 首先，需要有一个已经训练好的复杂模型，通常称为教师模型。这个教师模型通常具有更大的参数数量和更强大的性能。这个模型将被用来传递知识给学生模型。

2. **准备学生模型：** 接下来，准备一个更小的、待训练的学生模型。学生模型通常比教师模型具有更少的参数和较低的复杂度。

3. **定义目标函数：** 在知识蒸馏中，通常会定义一个目标函数，用于测量学生模型的预测和教师模型的预测之间的相似度。这个目标函数可以是交叉熵（Cross-Entropy）损失函数或其他损失函数，用来衡量学生模型的输出概率分布与教师模型的输出概率分布之间的差异。

4. **蒸馏训练：** 使用带有目标函数的训练数据集对学生模型进行训练。训练的目标是使学生模型的输出尽量接近教师模型的输出。通常，训练数据集可以是与原始任务相同的数据集，也可以是包含教师模型生成的软标签的数据集。

5. **温度参数：** 在知识蒸馏中，通常引入一个称为"温度"的超参数。温度参数控制了教师模型的输出分布在传递给学生模型之前的平滑程度。较高的温度值会使分布更平滑，而较低的温度值会使分布更尖锐。温度参数可以根据任务的性质进行调整。

6. **推理和应用：** 完成知识蒸馏训练后，学生模型就可以用于实际的推理和应用。学生模型通常比教师模型更轻量级，因此更适合部署在资源有限的设备上。

总的来说，知识蒸馏是一种用于模型压缩的技术，通过传递教师模型的知识来训练更小的学生模型。这种方法可以帮助在减小模型大小的同时保持模型性能，适用于移动设备、边缘计算和其他资源受限的环境。





## Transformer

https://blog.csdn.net/benzhujie1245com/article/details/117173090

其相对RNN网络结构最大的有点是可以并行计算。

![img](https://img-blog.csdnimg.cn/2021052223091261.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JlbnpodWppZTEyNDVjb20=,size_16,color_FFFFFF,t_70)

 Transformer 本质上是一个 Encoder-Decoder 架构。因此中间部分的 Transformer 可以分为两个部分：编码组件和解码组件。

论文中是由六个编码器和六个解码器组成

![img](https://img-blog.csdnimg.cn/2021052223173542.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JlbnpodWppZTEyNDVjb20=,size_16,color_FFFFFF,t_70)

每个编码器由两个子层组成：[Self-Attention](https://so.csdn.net/so/search?q=Self-Attention&spm=1001.2101.3001.7020) 层（自注意力层）和 Position-wise Feed Forward Network（前馈网络，缩写为 FFN）。每个编码器的结构都是相同的，但是它们使用不同的权重参数。





### 自注意力机制，Q、K、V的计算

假设有一个输入序列x，长度为n，每个位置的维度为d。假设我们希望计算x的自注意力机制输出y，也是一个长度为n，每个位置的维度为d的序列。下面我们以一个简单的示例来说明Q、K和V矩阵的关系：   假设我们有一个输入序列x，如下所示：

```
x = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
```

\1. 我们首先需要将x进行线性变换，得到Q、K和V三个矩阵：其中，Wq、Wk和Wv是可学习的权重矩阵。  

```
Q = x * Wq
K = x * Wk
V = x * Wv
```

2. 接下来，我们需要计算Q和K之间的相似度矩阵S，以及将S进行softmax操作，得到权重矩阵A：

```
S = Q * K^T
A = softmax(S)
```

\3. 然后，我们需要利用A对V进行加权求和，得到输出y：

```
y = A * V
```

其中，*表示矩阵乘法运算。   以上就是自注意力机制的计算过程，可以看到，Q、K和V矩阵在计算相似度矩阵和输出时都扮演着不同的角色，它们是注意力机制的核心。



## 图像分类算法

### VIT

https://blog.csdn.net/weixin_42392454/article/details/122667271?spm=1001.2014.3001.5502

![ViT-B16](https://img-blog.csdnimg.cn/969de854c7d24db58de94dbfbe1d89e4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA54Ot6KGA5Y6o5biI6ZW_,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



### EfficientNet和EfficientNetV2

https://blog.csdn.net/qq_37541097/article/details/116933569

EfficientNet结构（图为B0，分B0-B7）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201206235626380.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3MTIwMTI=,size_16,color_FFFFFF,t_70)

EfficientNetV2结构（分s,m,l）：

![EfficientNetv2-s](https://img-blog.csdnimg.cn/20210517182754449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center)



MBConv：

https://blog.csdn.net/qq_42057562/article/details/118513829

Fused-MBConv没有Depthwise-Conv2d和SE

![img](https://pic3.zhimg.com/80/v2-bfdbf57876928e549d9235a0f49d6526_720w.webp)





## YOLO

相当于将图片预先划分出几个区域，而再使用滑动窗口等方法获取候选区域。

一张图中有多个目标

每个区域由一个5x1向量表示（c, x, y, w, h）

![acbeeb835e281d3ad5fa45cd2a4b3d5c.png](https://img-blog.csdnimg.cn/img_convert/acbeeb835e281d3ad5fa45cd2a4b3d5c.png)



为什么这样子更优？因为conv操作是位置强相关的，就是原来的目标在哪里，你conv之后的feature map上还在哪里，所以图片划分为16个区域，结果也应该分布在16个区域上，所以我们的**结果(Tensor)的维度size是：(5,4,4)**。

![6b85c6979409715f7851f3c0bb71a3e4.png](https://img-blog.csdnimg.cn/img_convert/6b85c6979409715f7851f3c0bb71a3e4.png)

现在还是二分类问题，只是判断有没有头在我们划分的框中

但是你发现7个葫芦娃只有6个1，原因是某一个grid里面有2个目标，确实如此，第三行第三列的grid既有**水娃**又有**隐身娃**。这种一个区域有多个目标的情况我们**目前没法解决，因为我们的模型现在能力就这么大，只能在一个区域中检测出一个目标，如何改进我们马上就讨论，你可以现在先自己想一想。**





### 损失函数

```python3
loss = 0
for img in img_all:
   for i in range(4):
      for j in range(4):
         loss_ij = lamda_1*(c_pred-c_label)**2 + c_label*(x_pred-x_label)**2 +\
                     c_label*(y_pred-y_label)**2 + c_label*(w_pred-w_label)**2 + \
                     c_label*(h_pred-h_label)**2
         loss += loss_ij
loss.backward()
```

> 遍历所有图片，遍历所有位置，计算loss。

如何找到我的目标：NMS



### 多类型目标检测

img cbrp16 cbrp32 cbrp64 cbrp128 ... fc256-fc[5+2]xN（[c,x,y,w,h,one-hot]xN）

相当于再多个one-hot用来表示它是哪个目标类型

![f81039177004a41195a35d589d94943f.png](https://img-blog.csdnimg.cn/img_convert/f81039177004a41195a35d589d94943f.png)

损失函数

```
loss = 0
for img in img_all:
   for i in range(3):
      for j in range(4):
         c_loss = lamda_1*(c_pred-c_label)**2
         geo_loss = c_label*(x_pred-x_label)**2 +\
                     c_label*(y_pred-y_label)**2 + c_label*(w_pred-w_label)**2 + \
                     c_label*(h_pred-h_label)**2
         class_loss = 1/m * mse_loss(p_pred, p_label)
         loss_ij =c_loss  + geo_loss + class_loss
         loss += loss_ij
loss.backward()
```





### 小目标检测

小目标总是检测不佳，所以我们专门设计神经元去拟合小目标。

对于每个区域，我们用2个五元组(c,x,y,w,h)，一个负责回归大目标，一个负责回归小目标，同样添加one-hot vector，one-hot就是[0,1],[1,0]这样子，来表示属于哪一类(葫芦娃的头or葫芦娃的葫芦)。

![17c9fea76a3f4fe296789058a13f3826.png](https://img-blog.csdnimg.cn/img_convert/17c9fea76a3f4fe296789058a13f3826.png)

损失函数

```text
loss = 0
for img in img_all:
   for i in range(3):
      for j in range(4):
         c_loss = lamda_1*(c_pred-c_label)**2
         geo_loss = c_label_big*(x_big_pred-x_big_label)**2 +\
                     c_label_big*(y_big_pred-y_big_label)**2 + c_label_big*(w_big_pred-w_big_label)**2 + \
                     c_label_big*(h_big_pred-h_big_label)**2 +\
                     c_label_small*(x_small_pred-x_small_label)**2 +\
                     c_label_small*(y_small_pred-y_small_label)**2 + c_label_small*(w_small_pred-w_small_label)**2 + \
                     c_label_small*(h_small_pred-h_small_label)**2
         class_loss = 1/m * mse_loss(p_pred, p_label)
         loss_ij =c_loss  + geo_loss + class_loss
         loss += loss_ij
         
loss.backward()
```





### YOLOv2

YOLO v1虽然快，但是预测的框不准确，很多目标找不到：

- **预测的框不准确：准确度不足。**

- **很多目标找不到：recall不足。**

  

之前YOLO v1直接预测x,y,w,h，范围比较大，现在我们想预测一个稍微小一点的值，来增加准确度。

不得不先介绍2个新概念：**基于grid的偏移量和基于anchor的偏移量**。什么意思呢？

**基于anchor的偏移量**的意思是，anchor的位置是固定的，**偏移量=目标位置-anchor的位置**。

**基于grid的偏移量**的意思是，grid的位置是固定的，**偏移量=目标位置-grid的位置**。



**YOLO v2在位置上不使用Anchor框，宽高上使用Anchor框。**相当于先生成一个Anchor框，再通过预测偏移量的方式挪这个框





### YOLOv3

https://blog.csdn.net/IanYue/article/details/126463674

基于DarkNet 53

为了解决识别小目标的问题，YOLOv3使用三个检测头对大、中、小目标分别进行检测

每个grid设置9个先验框，3个大的，3个中的，3个小的。

![9efefba1a90e5ba9e8615fa30e9405af.png](https://img-blog.csdnimg.cn/img_convert/9efefba1a90e5ba9e8615fa30e9405af.png)



我们发现3个分支分别为**32倍下采样，16倍下采样，8倍下采样**，分别取预测**大，中，小目标**。为什么这样子安排呢？

因为**32倍下采样**每个点感受野更大，所以去预测**大目标，8倍下采样**每个点感受野最小，所以去预测**小目标。专人专事。**

**发现预测得更准确了，性能又提升了。**



每个分支预测3个框，每个框预测5元组+80个one-hot vector类别，所以一共size是：

**3\*(4+1+80)**

**每个分支的输出size为：**

- **[13,13,3\*(4+1+80)]**
- **[26,26,3\*(4+1+80)]**
- **[52,52,3\*(4+1+80)]**

![a226f8e9b8d4f7aa7d7179aeafc39f0c.png](https://img-blog.csdnimg.cn/img_convert/a226f8e9b8d4f7aa7d7179aeafc39f0c.png)

原文链接：https://blog.csdn.net/qq_38375203/article/details/125505508

● DBL：代表卷积、BN及Leaky ReLU三层的组合，在YOLOv3中卷积都是以这样的组合出现的，构成了DarkNet的基本单元。DBL后面的数字代表有几个DBL模块。
● res：res代表残差模块，res后面的数字代表有几个串联的残差模块。
● 上采样：上采样使用的方式为上池化，即元素复制扩充的方法使得特征图尺寸扩大，没有学习参数。
● Concat：上采样后将深层与浅层的特征图进行Concat操作，即通道的拼接，类似于FPN，但FPN使用的是逐元素相加。
● 残差思想：DarkNet-53借鉴了ResNet的残差思想，在基础网络中大量使用了残差连接，因此网络结构可以设计的很深，并且缓解了训练中梯度消失的问题，使得模型更容易收敛。
● 多层特征图：通过上采样与Concat操作，融合了深、浅层的特征，最终输出了3种尺寸的特征图，用于后续检测。多层特征图对于多尺度物体及小物体检测是有利的。
● 无池化层：之前的YOLO网络有5个最大池化层，用来缩小特征图的尺寸，下采样率为32，而DarkNet-53并没有采用池化的做法，而是通过步长为2的卷积核来达到缩小尺寸的效果，下采样次数同样是5次，总体下采样率为32。



![e8709b5b96c15e0b8d0e75672a8cfd3b.png](https://img-blog.csdnimg.cn/img_convert/e8709b5b96c15e0b8d0e75672a8cfd3b.png)



- **损失函数为：**

![3c7911fc8d07c47352cc2cd3998a536f.png](https://img-blog.csdnimg.cn/img_convert/3c7911fc8d07c47352cc2cd3998a536f.png)

YOLO v3损失函数

第4行说明：loss分3部分组成。

第1行代表geo_loss，S代表13,26,52，就是grid是几乘几的。B=5。

第2行代表class_loss，和YOLO v2的区别是改成了交叉熵。



### YOLOv4

各种优化方式的缝合怪算法

https://blog.csdn.net/weixin_43702653/article/details/124260237

BOF:

```
                    1）数据增强：图像几何变换（随机缩放，裁剪，旋转），Cutmix，Mosaic等
                    2）网络正则化：Dropout,Dropblock等
                    3） 损失函数的设计：边界框回归的损失函数的改进 CIOU               
```

BOS

```
                   1）增大模型感受野：SPP、ASPP等
                   2）引入注意力机制：SE、SAM
                   3）特征集成：PAN，BiFPN                                  
                   4）激活函数改进：Swish、Mish
                   5）后处理方法改进：soft NMS、DIoU NMS
```

BackBone主干网络：各种方法技巧结合起来，包括：CSPDarknet53、Mish激活函数、Dropblock

Neck：目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构

Head：输出层的锚框机制和Yolov3相同，主要改进的是训练时的回归框位置损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms



输入端Mosaic数据增强

​	**随机缩放、随机裁剪、随机排布**的方式进行拼接。参考cutMix，把四张图片拼接成一张图片做训练

	解决目标出现次数不均的问题，在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。
丰富数据集，减小batch



## PyTorch

### nn.Identiy()

https://blog.csdn.net/Z2572862506/article/details/129136354

f(x) = x

- 这个函数相当于输入什么就输出什么, 可以用在对已经设计好模型结构的修改, 比如模型的最后一层是 1000 分类, 我们可以将最后一层用 nn.Identity( ) 替换掉, 得到它之前学习的特征, 然后再自己设计最后一层的结构

- 在[迁移学习](https://so.csdn.net/so/search?q=迁移学习&spm=1001.2101.3001.7020)中经常使用

  ```python
  # 替代最后一层的全连接网络
  self.backbone.head.fc = nn.Identity()
  ```



### nn.parameter()





### nn.Dropout()

https://blog.csdn.net/weixin_42392454/article/details/113768868?spm=1001.2014.3001.5502

提高神经网络的泛化能力，减轻过拟合。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210209122356963.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5MjQ1NA==,size_16,color_FFFFFF,t_70#pic_center)

防止过度依赖某一个神经元，导致其他神经元相当于不起作用。

例如在人脸识别中，网络每次都只通过眼睛来判断是不是个人。如果此时直接拿一个全新的测试数据集让这个网络进行识别，很可能因为图片中眼睛被遮挡而得出错误的结果。这就是典型的过拟合。

像图(b)一样，随机失活几个神经元，迫使网络要让更多的神经元发挥作用，网络不仅识别人眼特征，而且还会识别嘴巴，鼻子等。这样在面对测试集时，即使训练数据和测试数据不同，准确率也不会太低。

dropout正则化方法广泛用于全连接层，因为卷积层基于像素的drop影响很小，依然会通过别的像素判断特征，卷积层用DropBlock(block_size=1时DropBlock退化成dropout)。

DropBlock步骤

1. 在特征图上随机选出两个点（和dropout一样的操作）

2. 以这个点为中心，扩散固定大小的边长，然后把这个矩形块的神经元失活

   



## TensorRT

https://blog.csdn.net/qq_37541097/article/details/114847600

### ONNX

用于不同框架间的模型转换

ONNX全称为Open Neural Network Exchange，是一种用于在深度学习框架之间交换模型的开放式格式。

ONNX可以被看作是一种中间结构，**它不能直接用于进行深度学习模型的推理**。在使用ONNX转换深度学习模型后，还需要将其转换为某个指定的结构

![在这里插入图片描述](https://img-blog.csdnimg.cn/08633515a4b0487b960e64aee1652b69.png)

#### pytorch模型转ONNX

```python
import torch
import torch.onnx
import onnx
import onnxruntime
import numpy as np
from torchvision.models import resnet34
from model import efficientnetv2_s as create_model
device = torch.device("cpu")


def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()


def main():
    weights_path = "/home/zee001-w/1TB_DISK/Codes/efficientnetV2/weights/model-0.pth"
    onnx_file_name = "weather.onnx"
    batch_size = 1
    img_h = 224
    img_w = 224
    img_channel = 3

    # create model and load pretrain weights
    model = create_model(num_classes=5)
    model.load_state_dict(torch.load(weights_path, map_location='cpu'))

    model.eval()
    # input to the model
    # 基于[batch, channel, height, width] 生成随机数，相当于随机生成一个输入
    x = torch.rand(batch_size, img_channel, img_h, img_w, requires_grad=True)
    # 记录一下原模型的精度，用于后续和转换成ONNX的模型做精度对比
    torch_out = model(x)

    # export the model
    torch.onnx.export(model,             # model being run
                      x,                 # model input (or a tuple for multiple inputs)
                      onnx_file_name,    # where to save the model (can be a file or file-like object)
                      input_names=["input"],
                      output_names=["output"],
                      verbose=False)

    # check onnx model invalid will raise an error
    onnx_model = onnx.load(onnx_file_name)
    onnx.checker.check_model(onnx_model)

    ort_session = onnxruntime.InferenceSession(onnx_file_name)

    # compute ONNX Runtime output prediction
    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}
    ort_outs = ort_session.run(None, ort_inputs)

    # compare ONNX Runtime and Pytorch results
    # assert_allclose: Raises an AssertionError if two objects are not equal up to desired tolerance.
    # 计算原模型和转换成ONNX的模型的相对误差和绝对误差，验证转换后模型精度
    # torch_out = {Tensor: (1, 5)} tensor([[ 0.0036,  0.8399, -0.2401, -0.4631, -0.0999]],\n       grad_fn=<AddmmBackward0>)
    # ort_outs = {list: 1} [array([[ 0.00356719,  0.8398867 , -0.24006702, -0.46306002, -0.09986615]],\n      dtype=float32)]
    np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)
    print("Exported model has been tested with ONNXRuntime, and the result looks good!")


if __name__ == '__main__':
    main()

```



#### ONNX转TensorRT

```shell
trtexec --onnx=resnet34.onnx --saveEngine=trt_output/resnet34.trt
```











## LSTM

LSTM动图

https://blog.csdn.net/steven_ysh/article/details/121964724

从RNN到LSTM再到GRU

https://blog.csdn.net/yjw123456/article/details/114358298

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210304102124448.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210304103156663.png)

ht=tanh(Whhht−1+Whxxt+bh)    ----->     ht=tanh(Wh[ht−1,xt]+bh)





LSTM是RNN的变种，是为了解决RNN存在的长期依赖问题而专门设计出来的。所谓长期依赖问题是，后面的单词在很长的时间序列后还依赖前面的单词，但由于梯度消失问题，导致前面的单词无法影响到后面的单词。



LSTM 引入了一个记忆单元（memory cell）和门控机制（gate mechanism），以解决这个问题。记忆单元可以选择性地存储和读取信息，门控机制包括遗忘门、输入门和输出门，它们可以控制信息的流动。这使得 LSTM 能够更好地处理长序列，选择性地记住和忘记信息，从而减轻了梯度消失的问题。



LSTM（长短时记忆网络）中的各个门及其作用如下：

1. 遗忘门（Forget Gate）：遗忘门的作用是**决定要从先前的记忆中遗忘哪些信息**。它接收当前时间步的输入和前一个时间步的隐藏状态作为输入，然后输出一个介于0和1之间的值，表示对先前的记忆有多少要被遗忘。如果遗忘门的输出接近于1，意味着保留之前的记忆；如果接近于0，意味着遗忘大部分之前的记忆。
2. 输入门（Input Gate）：输入门的作用是**决定哪些新的信息将被存储在记忆单元中**。它也接收当前时间步的输入和前一个时间步的隐藏状态作为输入，然后输出一个介于0和1之间的值，表示要将多少新信息添加到记忆单元中。
3. 更新记忆单元（Update Memory Cell）：在输入门决定哪些信息要存储后，**更新记忆单元用于计算新的记忆单元的内容**。它将当前时间步的输入和前一个时间步的隐藏状态作为输入，并结合输入门的输出来更新记忆单元。
4. 输出门（Output Gate）：输出门的作用是**根据当前的记忆单元来计算当前时间步的隐藏状态**。它接收当前时间步的输入和前一个时间步的隐藏状态作为输入，并结合记忆单元的内容来生成当前时间步的隐藏状态。



但是序列过长还是可能出现梯度爆炸或消失的问题，此时可以拼接多个LSTM或者梯度裁剪或者换模型
